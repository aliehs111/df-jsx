{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[paths] REPO_ROOT = /Users/sheilamcgovern/Desktop/Projects2025/df-jsx\n",
      "[paths] RAW_DATA_DIR     = /Users/sheilamcgovern/Desktop/Projects2025/df-jsx/notebooks/college_earnings/data\n",
      "[paths] OUTPUTS_DIR      = /Users/sheilamcgovern/Desktop/Projects2025/df-jsx/notebooks/college_earnings/outputs\n",
      "[paths] ARTIFACT_DIR     = /Users/sheilamcgovern/Desktop/Projects2025/df-jsx/server/routers/models/college_earnings/v1_75k_5y\n",
      "[paths] REPORT_PDF_PATH  = /Users/sheilamcgovern/Desktop/Projects2025/df-jsx/server/routers/models/college_earnings/v1_75k_5y/training_report.pdf\n",
      "[s3] USE_S3_UPLOAD=False  bucket=your-bucket-name  prefix=models/college_earnings/v1_75k_5y/\n",
      "[19:49:18] Notebook bootstrap complete. Proceed to data ingest…\n"
     ]
    }
   ],
   "source": [
    "# --- College Earnings Predictor: notebook bootstrap (Cell 1) ---\n",
    "# Purpose: make the notebook portable, set constants, and prep artifact paths.\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, sys, json, datetime as dt\n",
    "from pathlib import Path\n",
    "\n",
    "# ↳ 1) Find the repo root (df-jsx) no matter where Jupyter was launched\n",
    "def find_repo_root(start: Path = Path.cwd()) -> Path:\n",
    "    p = start.resolve()\n",
    "    while p != p.parent:\n",
    "        # heuristics: both server/routers and client/ exist in the project root\n",
    "        if (p / \"server\" / \"routers\").exists() and (p / \"client\").exists():\n",
    "            return p\n",
    "        p = p.parent\n",
    "    return start.resolve()\n",
    "\n",
    "REPO_ROOT = find_repo_root()\n",
    "print(f\"[paths] REPO_ROOT = {REPO_ROOT}\")\n",
    "\n",
    "# Ensure repo root is importable if you want local modules\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_ROOT))\n",
    "\n",
    "# ↳ 2) Notebook-local data directories (raw downloads & scratch)\n",
    "NB_DIR = REPO_ROOT / \"notebooks\" / \"college_earnings\"\n",
    "RAW_DATA_DIR = NB_DIR / \"data\"\n",
    "OUTPUTS_DIR = NB_DIR / \"outputs\"\n",
    "RAW_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ↳ 3) Model identifiers & constants (edit as needed)\n",
    "MODEL_NAME   = \"college_earnings\"\n",
    "VERSION      = \"v1_75k_5y\"      # keep lowercase \"k\" to match artifact folder name\n",
    "HORIZON      = \"p6\"             # ~6 years (proxy for 5–6y)\n",
    "TARGET_USD   = 75_000           # threshold for ≥ $75k\n",
    "RANDOM_SEED  = 42\n",
    "\n",
    "# ↳ 4) Artifact directory (where the FastAPI route already looks)\n",
    "ARTIFACT_DIR = REPO_ROOT / \"server\" / \"routers\" / \"models\" / MODEL_NAME / VERSION\n",
    "ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Optional: training report filename to export later via nbconvert\n",
    "REPORT_PDF_PATH = ARTIFACT_DIR / \"training_report.pdf\"\n",
    "\n",
    "print(f\"[paths] RAW_DATA_DIR     = {RAW_DATA_DIR}\")\n",
    "print(f\"[paths] OUTPUTS_DIR      = {OUTPUTS_DIR}\")\n",
    "print(f\"[paths] ARTIFACT_DIR     = {ARTIFACT_DIR}\")\n",
    "print(f\"[paths] REPORT_PDF_PATH  = {REPORT_PDF_PATH}\")\n",
    "\n",
    "# ↳ 5) Helpers for consistent saving/logging\n",
    "def save_json(data: dict, path: Path) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    path.write_text(json.dumps(data, indent=2))\n",
    "    print(f\"[save] {path.relative_to(REPO_ROOT)} ({path.stat().st_size} bytes)\")\n",
    "\n",
    "def utcnow() -> str:\n",
    "    return dt.datetime.utcnow().replace(microsecond=0).isoformat() + \"Z\"\n",
    "\n",
    "def log(msg: str) -> None:\n",
    "    print(f\"[{dt.datetime.now().strftime('%H:%M:%S')}] {msg}\")\n",
    "\n",
    "# ↳ 6) Planned artifact filenames (for later cells to use)\n",
    "ENCODERS_JSON      = ARTIFACT_DIR / \"encoders.json\"\n",
    "FIXED_EFFECTS_JSON = ARTIFACT_DIR / \"fixed_effects.json\"\n",
    "RAND_STATE_JSON    = ARTIFACT_DIR / \"random_state.json\"\n",
    "RAND_CIP_JSON      = ARTIFACT_DIR / \"random_cip.json\"\n",
    "CALIB_JSON         = ARTIFACT_DIR / \"calibration.json\"\n",
    "THRESHOLDS_JSON    = ARTIFACT_DIR / \"thresholds.json\"\n",
    "METADATA_JSON      = ARTIFACT_DIR / \"metadata.json\"\n",
    "\n",
    "# ↳ 7) (Optional) S3 settings if you later want to upload from the notebook\n",
    "USE_S3_UPLOAD = bool(int(os.getenv(\"EARNINGS_USE_S3_UPLOAD\", \"0\")))  # set 1 to enable\n",
    "S3_BUCKET     = os.getenv(\"EARNINGS_S3_BUCKET\", \"your-bucket-name\")\n",
    "S3_PREFIX     = f\"models/{MODEL_NAME}/{VERSION}/\"\n",
    "\n",
    "print(f\"[s3] USE_S3_UPLOAD={USE_S3_UPLOAD}  bucket={S3_BUCKET}  prefix={S3_PREFIX}\")\n",
    "\n",
    "# Sanity ping\n",
    "log(\"Notebook bootstrap complete. Proceed to data ingest…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:49:28] FoS ZIP: Most-Recent-Cohorts-Field-of-Study_04172025.zip\n",
      "[19:49:28] FoS CSV: Most-Recent-Cohorts-Field-of-Study.csv\n",
      "[19:49:30] Institution ZIP: Most-Recent-Cohorts-Institution_05192025.zip\n",
      "[19:49:30] Institution CSV: Most-Recent-Cohorts-Institution_05192025.csv\n",
      "[19:49:30] Rows after join/filter: 39,051\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unitid</th>\n",
       "      <th>instnm</th>\n",
       "      <th>control</th>\n",
       "      <th>cip4</th>\n",
       "      <th>ciptitle</th>\n",
       "      <th>credlev</th>\n",
       "      <th>creddesc</th>\n",
       "      <th>countoverall</th>\n",
       "      <th>earn_median</th>\n",
       "      <th>degree_level</th>\n",
       "      <th>public_private</th>\n",
       "      <th>state</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>100654.0</td>\n",
       "      <td>Alabama A &amp; M University</td>\n",
       "      <td>Public</td>\n",
       "      <td>1410</td>\n",
       "      <td>Electrical, Electronics and Communications Eng...</td>\n",
       "      <td>3</td>\n",
       "      <td>Bachelor's Degree</td>\n",
       "      <td>33.0</td>\n",
       "      <td>90409.0</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>Public</td>\n",
       "      <td>AL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>100654.0</td>\n",
       "      <td>Alabama A &amp; M University</td>\n",
       "      <td>Public</td>\n",
       "      <td>1419</td>\n",
       "      <td>Mechanical Engineering.</td>\n",
       "      <td>3</td>\n",
       "      <td>Bachelor's Degree</td>\n",
       "      <td>41.0</td>\n",
       "      <td>82929.0</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>Public</td>\n",
       "      <td>AL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>100654.0</td>\n",
       "      <td>Alabama A &amp; M University</td>\n",
       "      <td>Public</td>\n",
       "      <td>1499</td>\n",
       "      <td>Engineering, Other.</td>\n",
       "      <td>5</td>\n",
       "      <td>Master's Degree</td>\n",
       "      <td>30.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Master</td>\n",
       "      <td>Public</td>\n",
       "      <td>AL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      unitid                    instnm control  cip4  \\\n",
       "29  100654.0  Alabama A & M University  Public  1410   \n",
       "30  100654.0  Alabama A & M University  Public  1419   \n",
       "31  100654.0  Alabama A & M University  Public  1499   \n",
       "\n",
       "                                             ciptitle  credlev  \\\n",
       "29  Electrical, Electronics and Communications Eng...        3   \n",
       "30                            Mechanical Engineering.        3   \n",
       "31                                Engineering, Other.        5   \n",
       "\n",
       "             creddesc  countoverall  earn_median degree_level public_private  \\\n",
       "29  Bachelor's Degree          33.0      90409.0     Bachelor         Public   \n",
       "30  Bachelor's Degree          41.0      82929.0     Bachelor         Public   \n",
       "31    Master's Degree          30.0          NaN       Master         Public   \n",
       "\n",
       "   state  target  \n",
       "29    AL       1  \n",
       "30    AL       1  \n",
       "31    AL       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Data Ingest (FoS + Institution join) ---\n",
    "\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# 0) find FoS ZIP (you already have it)\n",
    "fos_zips = sorted([p for p in RAW_DATA_DIR.glob(\"*.zip\") if \"Field-of-Study\" in p.name or \"Field_of_Study\" in p.name],\n",
    "                  key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "assert fos_zips, \"Put the 'Most-Recent-Cohorts-Field-of-Study_*.zip' in notebooks/college_earnings/data/\"\n",
    "fos_zip = fos_zips[0]\n",
    "log(f\"FoS ZIP: {fos_zip.name}\")\n",
    "\n",
    "# 1) read FoS CSV (pick the largest CSV inside)\n",
    "with zipfile.ZipFile(fos_zip, \"r\") as zf:\n",
    "    fos_csv = max([m for m in zf.namelist() if m.lower().endswith(\".csv\")],\n",
    "                  key=lambda m: zf.getinfo(m).file_size)\n",
    "    log(f\"FoS CSV: {fos_csv}\")\n",
    "    with zf.open(fos_csv) as f:\n",
    "        peek = pd.read_csv(f, nrows=5)\n",
    "        fos_cols = {c.upper(): c for c in peek.columns}\n",
    "\n",
    "# aliases for FoS\n",
    "def has(colnames):  # returns original-case name if present\n",
    "    for c in colnames:\n",
    "        if c.upper() in fos_cols:\n",
    "            return fos_cols[c.upper()]\n",
    "    return None\n",
    "\n",
    "fos_use = {\n",
    "    \"UNITID\":      has([\"UNITID\"]),\n",
    "    \"INSTNM\":      has([\"INSTNM\"]),\n",
    "    \"CIPCODE\":     has([\"CIPCODE\"]),\n",
    "    \"CIPTITLE\":    has([\"CIPDESC\",\"CIPTITLE\"]),\n",
    "    \"CREDLEV\":     has([\"CREDLEV\"]),\n",
    "    \"CREDDESC\":    has([\"CREDDESC\"]),\n",
    "    \"CONTROL\":     has([\"CONTROL\"]),\n",
    "    \"COUNT\":       has([\"COUNTOVERALL\",\"IPEDSCOUNT1\",\"IPEDSCOUNT2\"]),\n",
    "    \"EARN_5YR\":    has([\"EARN_MDN_5YR\"]),\n",
    "    \"EARN_4YR\":    has([\"EARN_MDN_4YR\",\"EARN_MDN_HI_4YR\"]),\n",
    "    \"EARN_2YR\":    has([\"EARN_MDN_HI_2YR\"]),\n",
    "    \"EARN_1YR\":    has([\"EARN_MDN_HI_1YR\",\"EARN_MDN_1YR\"]),\n",
    "}\n",
    "\n",
    "earn_col = fos_use[\"EARN_5YR\"] or fos_use[\"EARN_4YR\"] or fos_use[\"EARN_2YR\"] or fos_use[\"EARN_1YR\"]\n",
    "assert fos_use[\"UNITID\"] and fos_use[\"CIPCODE\"] and fos_use[\"CREDLEV\"], \"Missing UNITID/CIPCODE/CREDLEV in FoS.\"\n",
    "assert earn_col, \"No earnings column found (looked for EARN_MDN_5YR/4YR/2YR/1YR).\"\n",
    "\n",
    "with zipfile.ZipFile(fos_zip, \"r\") as zf, zf.open(fos_csv) as f:\n",
    "    usecols = [v for v in [fos_use[\"UNITID\"], fos_use[\"INSTNM\"], fos_use[\"CIPCODE\"], fos_use[\"CIPTITLE\"],\n",
    "                           fos_use[\"CREDLEV\"], fos_use[\"CREDDESC\"], fos_use[\"CONTROL\"],\n",
    "                           fos_use[\"COUNT\"], earn_col] if v]\n",
    "    fos = pd.read_csv(f, usecols=usecols, low_memory=False)\n",
    "\n",
    "# normalize FoS\n",
    "rename_map = {}\n",
    "if fos_use[\"UNITID\"]:   rename_map[fos_use[\"UNITID\"]] = \"unitid\"\n",
    "if fos_use[\"INSTNM\"]:   rename_map[fos_use[\"INSTNM\"]] = \"instnm\"\n",
    "if fos_use[\"CIPCODE\"]:  rename_map[fos_use[\"CIPCODE\"]] = \"cip4\"\n",
    "if fos_use[\"CIPTITLE\"]: rename_map[fos_use[\"CIPTITLE\"]] = \"ciptitle\"\n",
    "if fos_use[\"CREDLEV\"]:  rename_map[fos_use[\"CREDLEV\"]] = \"credlev\"\n",
    "if fos_use[\"CREDDESC\"]: rename_map[fos_use[\"CREDDESC\"]] = \"creddesc\"\n",
    "if fos_use[\"CONTROL\"]:  rename_map[fos_use[\"CONTROL\"]] = \"control\"\n",
    "if fos_use[\"COUNT\"]:    rename_map[fos_use[\"COUNT\"]] = \"countoverall\"\n",
    "rename_map[earn_col] = \"earn_median\"\n",
    "\n",
    "fos = fos.rename(columns=rename_map)\n",
    "fos[\"cip4\"] = fos[\"cip4\"].astype(str).str.replace(r\"[^0-9]\", \"\", regex=True).str[:4]\n",
    "fos[\"earn_median\"] = pd.to_numeric(fos[\"earn_median\"], errors=\"coerce\")\n",
    "\n",
    "# 2) map labels used in your app\n",
    "cred_map = {2: \"Associate\", 3: \"Bachelor\", 5: \"Master\", 7: \"Professional\", 6: \"Doctoral\"}\n",
    "fos[\"degree_level\"] = fos[\"credlev\"].map(cred_map)\n",
    "control_map = {1: \"Public\", 2: \"Private Nonprofit\", 3: \"Private For-profit\"}\n",
    "\n",
    "\n",
    "\n",
    "# CONTROL can be numeric (1/2/3) or strings (\"Public\", \"Private Nonprofit\", etc.)\n",
    "ctrl_num = pd.to_numeric(fos[\"control\"], errors=\"coerce\")\n",
    "ctrl_str = fos[\"control\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "pp_from_num = np.where(ctrl_num == 1, \"Public\",\n",
    "                np.where(ctrl_num.isin([2, 3]), \"Private\", np.nan))\n",
    "\n",
    "pp = np.where(ctrl_num.notna(), pp_from_num,\n",
    "     np.where(ctrl_str.str.contains(\"public\", na=False), \"Public\",\n",
    "     np.where(ctrl_str.str.contains(\"private\", na=False), \"Private\", np.nan)))\n",
    "\n",
    "fos[\"public_private\"] = pd.Series(pp, index=fos.index)\n",
    "\n",
    "\n",
    "# 3) load Institution \"Most Recent\" to get state\n",
    "inst_zips = sorted([p for p in RAW_DATA_DIR.glob(\"*.zip\")\n",
    "                    if \"Institution\" in p.name or \"Most-Recent-Institution\" in p.name or \"Most-Recent-Cohorts-Institution\" in p.name],\n",
    "                   key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "assert inst_zips, \"Download the 'Most Recent' Institution ZIP to RAW_DATA_DIR as well (for STABBR).\"\n",
    "inst_zip = inst_zips[0]\n",
    "log(f\"Institution ZIP: {inst_zip.name}\")\n",
    "\n",
    "with zipfile.ZipFile(inst_zip, \"r\") as zf:\n",
    "    inst_csv = max([m for m in zf.namelist() if m.lower().endswith(\".csv\")],\n",
    "                   key=lambda m: zf.getinfo(m).file_size)\n",
    "    log(f\"Institution CSV: {inst_csv}\")\n",
    "    with zf.open(inst_csv) as f:\n",
    "        inst_peek = pd.read_csv(f, nrows=5)\n",
    "        inst_cols = {c.upper(): c for c in inst_peek.columns}\n",
    "\n",
    "stabbr_col = inst_cols.get(\"STABBR\") or inst_cols.get(\"STATE\")\n",
    "assert stabbr_col, \"Could not find STABBR/STATE in institution file.\"\n",
    "with zipfile.ZipFile(inst_zip, \"r\") as zf, zf.open(inst_csv) as f:\n",
    "    inst = pd.read_csv(f, usecols=[inst_cols[\"UNITID\"], stabbr_col])\n",
    "inst = inst.rename(columns={inst_cols[\"UNITID\"]: \"unitid\", stabbr_col: \"state\"})\n",
    "inst[\"state\"] = inst[\"state\"].astype(str).str.upper().str.strip()\n",
    "\n",
    "# 4) merge FoS + state\n",
    "df = fos.merge(inst, on=\"unitid\", how=\"left\")\n",
    "\n",
    "# 5) target & basic cohort filter\n",
    "df[\"target\"] = (df[\"earn_median\"] >= TARGET_USD).astype(\"Int64\")\n",
    "if \"countoverall\" in df.columns:\n",
    "    df = df[df[\"countoverall\"].fillna(0).astype(\"Int64\") >= 30].copy()\n",
    "\n",
    "log(f\"Rows after join/filter: {len(df):,}\")\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC (fixed):  0.711 | Brier: 0.167\n",
      "AUC (fx+grp): 0.893 | Brier: 0.113\n",
      "AUC (cal):    0.893 | Brier: 0.113\n",
      "[save] server/routers/models/college_earnings/v1_75k_5y/encoders.json (5228 bytes)\n",
      "[save] server/routers/models/college_earnings/v1_75k_5y/fixed_effects.json (622 bytes)\n",
      "[save] server/routers/models/college_earnings/v1_75k_5y/random_state.json (1634 bytes)\n",
      "[save] server/routers/models/college_earnings/v1_75k_5y/random_cip.json (10083 bytes)\n",
      "[save] server/routers/models/college_earnings/v1_75k_5y/calibration.json (138 bytes)\n",
      "[save] server/routers/models/college_earnings/v1_75k_5y/thresholds.json (33 bytes)\n",
      "[save] server/routers/models/college_earnings/v1_75k_5y/metadata.json (718 bytes)\n",
      "[19:49:37] Artifacts written. You can now hit /api/predictors/infer with real scores.\n"
     ]
    }
   ],
   "source": [
    "# --- Modeling + Calibration + Artifact Export (one cell) ---\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, brier_score_loss\n",
    "\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "# 0) Minimal sanity\n",
    "req = [\"degree_level\",\"state\",\"cip4\",\"public_private\",\"target\"]\n",
    "missing_cols = [c for c in req if c not in df.columns]\n",
    "assert not missing_cols, f\"Missing required columns: {missing_cols}\"\n",
    "dfm = df.dropna(subset=[\"degree_level\",\"state\",\"cip4\"]).copy()\n",
    "dfm[\"target\"] = dfm[\"target\"].astype(int)\n",
    "\n",
    "# 1) Fixed + group features\n",
    "# use countoverall as a light size proxy (optional)\n",
    "if \"countoverall\" in dfm.columns:\n",
    "    q = pd.qcut(dfm[\"countoverall\"].fillna(0), q=4, duplicates=\"drop\")\n",
    "    dfm[\"size_bin\"] = q.astype(str)\n",
    "else:\n",
    "    dfm[\"size_bin\"] = \"NA\"\n",
    "\n",
    "fixed_feats  = [\"degree_level\",\"public_private\",\"size_bin\"]\n",
    "group_feats  = [\"state\",\"cip4\"]\n",
    "\n",
    "# 2) Train/val split grouped by institution to reduce leakage\n",
    "groups = dfm.get(\"unitid\", pd.Series(range(len(dfm))))\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=RANDOM_SEED)\n",
    "(train_idx, val_idx), = gss.split(dfm, groups=groups)\n",
    "tr, va = dfm.iloc[train_idx].copy(), dfm.iloc[val_idx].copy()\n",
    "y_tr, y_va = tr[\"target\"].values, va[\"target\"].values\n",
    "\n",
    "# 3) Baseline fixed-effects logistic (sanity)\n",
    "Xtr_fixed = pd.get_dummies(tr[fixed_feats], drop_first=False)\n",
    "Xva_fixed = pd.get_dummies(va[fixed_feats], drop_first=False).reindex(columns=Xtr_fixed.columns, fill_value=0)\n",
    "\n",
    "clf_fixed = LogisticRegression(penalty=\"l2\", C=1.0, max_iter=300, solver=\"liblinear\", random_state=RANDOM_SEED)\n",
    "clf_fixed.fit(Xtr_fixed, y_tr)\n",
    "p_va_fixed = clf_fixed.predict_proba(Xva_fixed)[:,1]\n",
    "print(f\"AUC (fixed):  {roc_auc_score(y_va, p_va_fixed):.3f} | Brier: {brier_score_loss(y_va, p_va_fixed):.3f}\")\n",
    "\n",
    "# 4) GLMM-ish with group dummies (state, cip4) + ridge-like shrink\n",
    "Xtr = pd.get_dummies(tr[fixed_feats + group_feats], drop_first=False)\n",
    "Xva = pd.get_dummies(va[fixed_feats + group_feats], drop_first=False).reindex(columns=Xtr.columns, fill_value=0)\n",
    "\n",
    "clf = LogisticRegression(penalty=\"l2\", C=0.5, max_iter=500, solver=\"liblinear\", random_state=RANDOM_SEED)\n",
    "clf.fit(Xtr, y_tr)\n",
    "p_va = clf.predict_proba(Xva)[:,1]\n",
    "print(f\"AUC (fx+grp): {roc_auc_score(y_va, p_va):.3f} | Brier: {brier_score_loss(y_va, p_va):.3f}\")\n",
    "\n",
    "# 5) Extract coefficients and split into fixed vs group parts\n",
    "coef = pd.Series(clf.coef_[0], index=Xtr.columns)\n",
    "intercept = float(clf.intercept_[0])\n",
    "\n",
    "fixed_cols = Xtr_fixed.columns\n",
    "group_state_cols = [c for c in Xtr.columns if c.startswith(\"state_\")]\n",
    "group_cip_cols   = [c for c in Xtr.columns if c.startswith(\"cip4_\")]\n",
    "\n",
    "fixed_coefs = coef.loc[fixed_cols].to_dict()\n",
    "\n",
    "# 6) Empirical-Bayes-ish shrinkage for random intercepts by group size\n",
    "def group_total(s, key_col, n_col):\n",
    "    if n_col not in s.columns: return s.groupby(key_col).size().rename(\"n\")\n",
    "    return s.groupby(key_col)[n_col].sum().rename(\"n\")\n",
    "\n",
    "state_sizes = group_total(tr, \"state\", \"countoverall\")\n",
    "cip_sizes   = group_total(tr, \"cip4\", \"countoverall\")\n",
    "\n",
    "def shrink(effect, n, k=200.0):\n",
    "    w = float(n) / float(n + k) if pd.notnull(n) else 0.0\n",
    "    return float(w * effect)\n",
    "\n",
    "random_state = {}\n",
    "for c in group_state_cols:\n",
    "    st = c.split(\"state_\",1)[1]\n",
    "    eff = coef[c]\n",
    "    n = state_sizes.get(st, 0.0)\n",
    "    random_state[st] = shrink(eff, n)\n",
    "\n",
    "random_cip = {}\n",
    "for c in group_cip_cols:\n",
    "    cp = c.split(\"cip4_\",1)[1]\n",
    "    eff = coef[c]\n",
    "    n = cip_sizes.get(cp, 0.0)\n",
    "    random_cip[cp] = shrink(eff, n)\n",
    "\n",
    "# 7) Simple Platt scaling on validation probs\n",
    "eps = 1e-8\n",
    "def logit(p): \n",
    "    p = np.clip(p, eps, 1-eps)\n",
    "    return np.log(p/(1-p))\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "platt = LR(penalty=None, max_iter=1000, solver=\"lbfgs\")\n",
    "\n",
    "platt.fit(logit(p_va).reshape(-1,1), y_va)\n",
    "\n",
    "def platt_calibrate(p):\n",
    "    z = logit(np.asarray(p)).reshape(-1,1)\n",
    "    return platt.predict_proba(z)[:,1]\n",
    "\n",
    "p_va_cal = platt_calibrate(p_va)\n",
    "print(f\"AUC (cal):    {roc_auc_score(y_va, p_va_cal):.3f} | Brier: {brier_score_loss(y_va, p_va_cal):.3f}\")\n",
    "\n",
    "# 8) Thresholds & encoders\n",
    "low_cut, high_cut = 0.33, 0.66\n",
    "\n",
    "encoders = {\n",
    "    \"degree_levels\": sorted(dfm[\"degree_level\"].dropna().unique().tolist()),\n",
    "    \"states\": sorted(dfm[\"state\"].dropna().unique().tolist()),\n",
    "    \"cip4\": sorted(dfm[\"cip4\"].dropna().unique().tolist()),\n",
    "    \"public_private\": sorted(dfm[\"public_private\"].dropna().unique().tolist()),\n",
    "    \"size_bins\": sorted(dfm[\"size_bin\"].dropna().unique().tolist()),\n",
    "    \"fixed_feature_columns\": list(fixed_cols)\n",
    "}\n",
    "\n",
    "fixed_effects = {\n",
    "    \"intercept\": intercept,\n",
    "    \"coefficients\": fixed_coefs\n",
    "}\n",
    "\n",
    "calibration = {\n",
    "    \"type\": \"platt\",\n",
    "    \"coef\": float(platt.coef_[0][0]),\n",
    "    \"intercept\": float(platt.intercept_[0]),\n",
    "    \"note\": \"Input is raw model probability logit.\"\n",
    "}\n",
    "\n",
    "thresholds = {\"low\": low_cut, \"high\": high_cut}\n",
    "\n",
    "metadata = {\n",
    "    \"model\": MODEL_NAME,\n",
    "    \"version\": VERSION,\n",
    "    \"target\": f\"Pr(median earnings ≥ ${TARGET_USD:,} at ~5 years; FoS 5-year median used if available)\",\n",
    "    \"trained_at\": utcnow(),\n",
    "    \"notes\": [\n",
    "        \"Fixed effects: degree_level, public_private, size_bin.\",\n",
    "        \"Group effects: state, cip4 via L2 + empirical shrink.\",\n",
    "        \"Calibration: Platt on validation grouped by UNITID split.\"\n",
    "    ],\n",
    "    \"counts\": {\n",
    "        \"train\": int(len(tr)),\n",
    "        \"valid\": int(len(va)),\n",
    "        \"pos_rate_train\": float(np.mean(y_tr)),\n",
    "        \"pos_rate_valid\": float(np.mean(y_va))\n",
    "    },\n",
    "    \"metrics_valid\": {\n",
    "        \"auc_fixed\": float(roc_auc_score(y_va, p_va_fixed)),\n",
    "        \"auc_fx_grp\": float(roc_auc_score(y_va, p_va)),\n",
    "        \"auc_cal\":    float(roc_auc_score(y_va, p_va_cal)),\n",
    "        \"brier_cal\":  float(brier_score_loss(y_va, p_va_cal))\n",
    "    }\n",
    "}\n",
    "\n",
    "# 9) Save artifacts\n",
    "save_json(encoders, ENCODERS_JSON)\n",
    "save_json(fixed_effects, FIXED_EFFECTS_JSON)\n",
    "save_json(random_state, RAND_STATE_JSON)\n",
    "save_json(random_cip, RAND_CIP_JSON)\n",
    "save_json(calibration, CALIB_JSON)\n",
    "save_json(thresholds, THRESHOLDS_JSON)\n",
    "save_json(metadata, METADATA_JSON)\n",
    "\n",
    "log(\"Artifacts written. You can now hit /api/predictors/infer with real scores.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[save] /Users/sheilamcgovern/Desktop/Projects2025/df-jsx/server/routers/models/college_earnings/v1_75k_5y/cip4_labels.json (21378 bytes)\n"
     ]
    }
   ],
   "source": [
    "# Build CIP4 -> label map (most common title per code)\n",
    "from collections import Counter\n",
    "import json, os\n",
    "\n",
    "# ensure codes are the same normalized 4-digit strings you use in artifacts\n",
    "fos[\"_cip4\"] = fos[\"cip4\"].astype(str).str.replace(r\"\\D\", \"\", regex=True).str[:4]\n",
    "\n",
    "label_map = {}\n",
    "for code, titles in fos.groupby(\"_cip4\")[\"ciptitle\"]:\n",
    "    title = Counter(titles.dropna().str.strip()).most_common(1)[0][0]\n",
    "    label_map[code] = title\n",
    "\n",
    "# optional: hand-fix a couple common names\n",
    "label_map.setdefault(\"1101\", \"Computer Science\")\n",
    "label_map.setdefault(\"5203\", \"Accounting\")\n",
    "\n",
    "labels_path = ARTIFACT_DIR / \"cip4_labels.json\"\n",
    "labels_path.write_text(json.dumps(label_map, ensure_ascii=False, indent=2))\n",
    "print(\"[save]\", labels_path, f\"({os.path.getsize(labels_path)} bytes)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
